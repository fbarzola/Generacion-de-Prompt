{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4126a17",
   "metadata": {},
   "source": [
    "# RAG System\n",
    "\n",
    "El tema abordado es la dificultad para obtener contextos precisos y relevantes a partir de grandes volúmenes de información. En muchas aplicaciones, como servicios de atención al cliente, educación o investigación, es crucial disponer de respuestas contextuales que integren información precisa y específica. Sin embargo, los sistemas actuales a menudo enfrentan problemas de relevancia y precisión cuando se les consulta sobre temas complejos o amplios. Implementar un sistema RAG que utilice prompts optimizados para generar contexto relevante es relevante porque mejora la precisión y utilidad de las respuestas generadas, ofreciendo resultados más completos y ajustados a la consulta.\n",
    "\n",
    "Este notebook implementa un sistema de Recuperación y Generación (RAG) basado en embeddings y FAISS para mejorar la busqueda de contexto basado en similitud de embeddings. El sistema recibirá consultas de parte del area comercial de una empresa relacionadas a las ventas y pagos, la idea es poder darle un contexto especifico al LLM basado en acciones las cuales pueden ser (getsales, getpaymentsapprove, getordersapprove, actionapprovepayments).\n",
    "\n",
    "En el caso de recibir una consulta general no relacionada con las ventas y los pagos de la empresa, la misma irá directo a ser procesada por el modelo de LLM sin recibir un contexto relacionado para dar respuesta.\n",
    "\n",
    "### Desarrollo de la propuesta de solución\n",
    "\n",
    "La solución propuesta es desarrollar un sistema RAG que emplee la generación de prompts en dos etapas: recuperación y generación. En la etapa de recuperación, un modelo de búsqueda seleccionará los documentos más relevantes de una base vectorial o indice vectorial, los cuales contendran los embeddings generados del archivo json que contiene la informacion para generar contextos, en la etapa de generación, un modelo de texto-texto creará respuestas contextuales basadas en la información recuperada.\n",
    "\n",
    "##### Prompt de ejemplo:\n",
    "\n",
    "• Texto-texto (para generación de contexto): “Usa la información de los documentos recuperados para responder a la pregunta: ¿Cuáles son las ventas aprobadas del mes de Enero?”\n",
    "\n",
    "Este prompt se optimizará para que el sistema pueda generar contextos detallados que incluyan las ideas principales y ejemplos relevantes de los documentos recuperados.\n",
    "\n",
    "\n",
    "### Viabilidad del proyecto \n",
    "\n",
    "El proyecto es técnicamente viable, ya que se basa en el uso de tecnologías accesibles como modelos de recuperación de información (por ejemplo, OpenSearch o FAISS) y modelos generativos de texto (GPT, Llama) . La generación y optimización de prompts permitirá un ajuste eficiente del sistema para mejorar la precisión del contexto generado. El proyecto puede \n",
    "desarrollarse en etapas, comenzando con la creación de prompts básicos y pruebas con conjuntos de datos limitados, seguido de la integración y pruebas de escalabilidad. Los recursos disponibles en plataformas de código abierto y servicios de nube facilitarán la implementación y prueba de la solución.\n",
    "\n",
    "### Incluye las siguientes funcionalidades:\n",
    "\n",
    "- Generacion de chunks y embeddings para llenar nuestro indice de busqueda.   \n",
    "- Creación de índices con FAISS.\n",
    "- Recuperación de contexto relevante.\n",
    "- Algoritmos de prediccion para busquedas de similitud de embeddings en el index.faiss\n",
    "- Generación de prompts contextuales a partir de consultas.\n",
    "- Conexion con API de OpenIA\n",
    "\n",
    "\n",
    "### EJEMPLOS DE CONSULTAS y SUS ACCIONES:\n",
    "\n",
    "- Quiero aprobar un pago = \"actionapprovepayments\"\n",
    "- Quiero las ventas de este mes = \"getsales\"\n",
    "- Quiero las ventas aprobadas = \"getordersapprove\"\n",
    "- Quiero los pagos aprobados de hoy = \"getpaymentsapprove\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88995c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu==1.7.3 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (1.7.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (3.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (0.26.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.21.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai==0.27.8 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.27.8) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.27.8) (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai==0.27.8) (4.64.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.8) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.8) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.8) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai==0.27.8) (2021.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai==0.27.8) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\fernando barzola\\appdata\\roaming\\python\\python39\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai==0.27.8) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai==0.27.8) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu==1.7.3\n",
    "!pip install sentence-transformers\n",
    "!pip install openai==0.27.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456155e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2478f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key de referencia\n",
    "\n",
    "openai.api_key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309af6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generador de embeddings\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def get_embedding(self, text: str):\n",
    "        return self.model.encode([text])\n",
    "\n",
    "    def get_document_embeddings(self, documents: list):\n",
    "        return self.model.encode(documents)\n",
    "\n",
    "embedding_model = EmbeddingModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a7ca71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesamiento del JSON\n",
    "\n",
    "def clean_json_content(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return json.loads(content)\n",
    "\n",
    "def read_actions_from_json(cleaned_data):\n",
    "    prompts = []\n",
    "    actions = []\n",
    "    tags = []\n",
    "    for action in cleaned_data.get('actionsParsed', []):\n",
    "        prompts.append(action['prompt'])\n",
    "        actions.append(action.get('name', 'unknown'))\n",
    "        tags.append(action.get('tag', '').split(', ') if action.get('tag') else [])\n",
    "    return prompts, actions, tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd57b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks e Índice FAISS\n",
    "\n",
    "def split_into_chunks(text, action, chunk_size=1024, overlap=50):\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_text = \" \".join(tokens[i:i + chunk_size])\n",
    "        chunks.append((chunk_text, action))\n",
    "    return chunks\n",
    "\n",
    "json_path = 'actions.json'\n",
    "cleaned_data = clean_json_content(json_path)\n",
    "documents, actions, tags = read_actions_from_json(cleaned_data)\n",
    "\n",
    "chunks = []\n",
    "for doc, action in zip(documents, actions):\n",
    "    doc_chunks = split_into_chunks(doc, action)\n",
    "    chunks.extend(doc_chunks)\n",
    "\n",
    "chunk_texts = [chunk[0] for chunk in chunks]\n",
    "chunk_embeddings = embedding_model.get_document_embeddings(chunk_texts)\n",
    "\n",
    "tag_texts = [tag for sublist in tags for tag in sublist]\n",
    "tag_embeddings = embedding_model.get_document_embeddings(tag_texts)\n",
    "\n",
    "all_embeddings = np.concatenate([chunk_embeddings, tag_embeddings])\n",
    "all_texts = chunk_texts + tag_texts\n",
    "all_actions = [chunk[1] for chunk in chunks] + [actions[i] for i, tag_list in enumerate(tags) for _ in tag_list]\n",
    "\n",
    "d = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "id_map = faiss.IndexIDMap(index)\n",
    "ids = np.arange(len(all_embeddings))\n",
    "id_map.add_with_ids(np.array(all_embeddings).astype('float32'), ids)\n",
    "faiss.write_index(id_map, \"index.faiss\")\n",
    "\n",
    "id_to_action = {i: all_actions[i] for i in range(len(all_actions))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa0bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, index_path: str, id_to_action: dict, documents: list, tags: dict):\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        self.id_to_action = id_to_action\n",
    "        self.documents = documents\n",
    "        self.tags = tags\n",
    "\n",
    "    def predict(self, query_embedding, top_k=3, threshold=0.7):\n",
    "        \n",
    "        if query_embedding.ndim == 1:\n",
    "            query_embedding = query_embedding.reshape(1, -1)\n",
    "        D, I = self.index.search(query_embedding, k=top_k)\n",
    "\n",
    "        results = []\n",
    "        for distance, idx in zip(D[0], I[0]):\n",
    "            if distance > threshold:\n",
    "                continue \n",
    "            action = self.id_to_action.get(idx, \"unknown\")\n",
    "            document = self.documents[idx] if idx < len(self.documents) else \"Tag relacionado\"\n",
    "            results.append((action, document))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def generate_context(self, query_embedding, query):\n",
    "        best_documents = self.predict(query_embedding, top_k=3, threshold=0.7)\n",
    "\n",
    "        if not best_documents:\n",
    "            print(\"\\n[DEBUG] No se encontró contexto relevante.\")\n",
    "            return None\n",
    "\n",
    "        relevant_texts = []\n",
    "        for action, _ in best_documents:\n",
    "            associated_chunks = [\n",
    "                doc for doc, act in zip(self.documents, self.id_to_action.values()) if act == action\n",
    "            ]\n",
    "            prompt = associated_chunks[0] if associated_chunks else \"Prompt no encontrado.\"\n",
    "            tag = self.tags.get(action, \"Sin tag relacionado\")\n",
    "            context = f\"Action: {action}\\nTag: {tag}\\nPrompt: {prompt}\"\n",
    "            relevant_texts.append(context)\n",
    "\n",
    "        return relevant_texts[0] if relevant_texts else None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a936e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline RAG\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, index_path: str, id_to_action: dict, documents: list, tags: dict, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.embedding = EmbeddingModel(model_name=embedding_model_name)\n",
    "        self.retriever = Retriever(index_path=index_path, id_to_action=id_to_action, documents=documents, tags=tags)\n",
    "\n",
    "    def generate_rag_prompt(self, query: str):\n",
    "        query_embedding = self.embedding.get_embedding(query).reshape(1, -1).astype('float32')\n",
    "        relevant_context = self.retriever.generate_context(query_embedding, query)\n",
    "\n",
    "        if relevant_context == \"No se encontró contexto relevante para la consulta.\":\n",
    "            return None\n",
    "        return f\"Consulta: {query}\\n\\nContexto:\\n{relevant_context}\"\n",
    "\n",
    "    def get_llm_response(self, query: str):\n",
    "        rag_prompt = self.generate_rag_prompt(query)\n",
    "\n",
    "        if not rag_prompt:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Eres un asistente general. Responde claramente a las consultas del usuario.\"},\n",
    "                        {\"role\": \"user\", \"content\": query}\n",
    "                    ],\n",
    "                    max_tokens=200,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                llm_response = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                return None, llm_response\n",
    "            except openai.OpenAIError as e:\n",
    "                print(f\"Error al llamar a la API de OpenAI: {e}\")\n",
    "                return None, \"Hubo un problema al obtener la respuesta del modelo.\"\n",
    "\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Eres un asistente que utiliza contexto proporcionado para responder consultas.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Consulta: {query}\\n\\nContexto:\\n{rag_prompt}\"}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            llm_response = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return rag_prompt, llm_response\n",
    "        except openai.OpenAIError as e:\n",
    "            print(f\"Error al llamar a la API de OpenAI: {e}\")\n",
    "            return rag_prompt, \"Hubo un problema al obtener la respuesta del modelo.\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f224d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresa tu consulta: quiero los pagos aprobados\n",
      "\n",
      "Contexto generado:\n",
      " Consulta: quiero los pagos aprobados\n",
      "\n",
      "Contexto:\n",
      "Action: getPaymentsApprove\n",
      "Tag: cuantos pagos se aprobaron hoy, quiero los pagos, quiero los pagos aprobados, quiero los pagos aprobados del ultimos mes\n",
      "Prompt: | getPaymentsApprove |: Esta accion se llama |getPaymentsApprove| permite visualizar los pagos que estan pendientes de aprobar. Importante -> Esta accion no requiere que solicites la fecha de inicio y fin. Los valores obtenidos en los parametros deben agregarse dentro de la key \"utterance\" . Solo repreguntar los valores de los parametros en caso de que alguno de ellos no se encuentre definido la key -> 'params' del json de la respuesta o que el valor de la key no se encuentre. En caso de tener que repreguntar al usuario, colocar la key -> \"followUp\": true , pero si todos los params se encuentran definidos establecer la key -> \"followUp\": false dentro de la respuesta del json. Si ya no es necesario repreguntar porque se obtuvo correctamente el action y los params enviar en la key -> 'text' un mensaje que informe al usuario que va recibir el resultado de lo que solicito, por ejemplo : Aqui tienes las ventas del mes de Enero para Leanval que solicitaste. Cuando hables de compañia es importante que realices el mapeo de la siguiente manera: Novatech -> \"company_id\" : 1 , Leanval -> \"company_id\" : 1 , Athuel -> \"company_id\" : 3 Para ejecutar la accion se debe obtener cada uno de los siguientes parametros: - company_id -> Este parametro debe identificar las compañías/empresas sobre las cuales se solicita información de ventas. Este parámetro debe ser un array que incluya los valores de las compañías mencionadas por el usuario. En caso de que se requiera información de todas las compañías/empresas, el array debe contener los valores de todas las compañías/empresas disponibles.\n",
      "\n",
      "Respuesta del LLM:\n",
      " Para poder obtener los pagos aprobados, necesito que me proporciones la identificación de la empresa (company_id). ¿De qué empresa necesitas la información?\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RAGPipeline(\n",
    "    index_path=\"index.faiss\",\n",
    "    id_to_action=id_to_action,\n",
    "    documents=all_texts,\n",
    "    tags={action: \", \".join(tags[i]) for i, action in enumerate(actions)}\n",
    ")\n",
    "\n",
    "query = input(\"Ingresa tu consulta: \")\n",
    "context, response = rag_pipeline.get_llm_response(query)\n",
    "\n",
    "print(\"\\nContexto generado:\\n\", context)\n",
    "\n",
    "print(\"\\nRespuesta del LLM:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb7230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e467ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
